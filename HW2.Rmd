---
title: "HW2_MVA"
output:
  word_document: default
  html_document: default
  pdf_document: default
date: "2025-12-14"
---

## Clean environment and load data

```{r}
if(!is.null(dev.list())) dev.off()
rm(list=ls())

library(readxl)
library(corrplot)
library(FactoMineR)
library(cluster)
library(ggplot2)
library(ggrepel)
library(kmed)
library(dplyr)
library(factoextra)

set.seed(123)
```

## 1. Exploratory data analysis

### Import the data set to R correctly and assign player names as rownames of the data frame.

The dataset has been imported with the \emph{read_excel()} function from the \emph{readxl} library. Using the \emph{as.data.frame()} function, it has been converted to a data frame. 

The row names of the data frame have been assigned as the players' names. Moreover, the column containing the players' names has been removed from the data frame since it is no longer needed after being set as row names.

```{r}
data <- read_excel("data_Eurobasket_2025.xlsx")
head(data)

df <- as.data.frame(data) # Convert to a regular data frame
rownames(df) <- df$PLAYER # Set the PLAYER column as row names
df <- df[, -which(names(df) == "PLAYER")] # We remove the PLAYER column (redundant)

# Rename variables with numbers
names(df)[names(df) == "2PT FG"] <- "TwoPt.FG"
names(df)[names(df) == "3PT FG"] <- "ThreePt.FG"

head(df)
```

The structure of the dataset has been checked using the \emph{str()} function. Categorical variables (\emph{Team} and \emph{Position}) have been converted to factors because they represent qualitative categories, while all other variables remained numerical.

```{r}
str(df)

# Convert categorical variables to factors
df$Team <- as.factor(df$Team)
df$Position <- as.factor(df$Position)
```

### Dataframe using variables “Position”, “MIN”, “FG”, “2PT.FG”, “3PT.FG”, “FT”, “OREB”, “DREB”, “REB”, “AST”, “PF”, “TO”,“STL”,“BLK”,“EFF” and “PTS”

A new data frame has been created based on the requested variables.

Moreover, the correlation matrix of the numeric variables in the new data frame has been computed. To visualize the correlation results, the \emph{corrplot()} function from the \emph{corrplot} library has been used.

The vast majority of numerical variables are positively correlated. In fact, those that are negatively correlated do not show a strong relationship. The highest negative correlation is between 2PT.FG and 3PT.FG, with value of -0.311. A player who takes a lot of 3-point shots may take fewer shots near the basket, and vice versa, but this trade-off is not extreme.

Nonetheless, there are some particular interesting positive correlations. To analyze these results more effectively, we will divide the study into three groups:

1) Very strong correlated variables (correlation > |0.90|):
- EFF and PTS (r = 0.95): Efficiency and points are almost perfectly correlated. These suggests that the more points a player scores, the more efficient is the player.

- DREB and REB (r = 0.97): Total rebounds is the sum of offensive rebounds and defensive rebounds. Nonetheless, it makes sense that defensive rebounds dominate total rebounds, since defensive rebounds are typically more frequent than offensive rebounds.

- EFF and DREB (r = 0.90): Players who grab defensive rebounds tend to have higher efficiency ratings.

2) Strong correlated variables (|0.90| > correlation > |0.80|):
- EFF and MIN (r = 0.83), MIN and PTS (r = 0.84): The more minutes a player spends on the court, the more points they tend to score, and, consequently, the higher their efficiency.

- OREB and REB (r = 0.87): Obviously, the total rebounds are correlated with offensive rebounds, since it contributes directly to the total number of rebounds.

- EFF and REB (r = 0.899): Usually, the more rebounds a player catches, the more efficient a player is.

- DREB and PTS (r = 0.817): The more defensive rebounds a player collects, the higher the probability of scoring, since each rebound creates an additional offensive opportunity, and therefore, more points can be scored.

3) Moderate correlated variables (|0.80| > correlation > |0.65|):

- AST and EFF (r = 0.666), EFF and TO (r = 0.761), AST and TO (r = 0.772): Efficiency is partly determined by assists and turnovers, so it makes sense that these variables are correlated. What is surprising, however, is that the correlation between efficiency and turnovers is positive, since a higher number of turnovers would normally be expected to reduce a player’s efficiency. On the other hand, the more assists a player makes, the more frequently they handle the ball, which naturally increases the likelihood of committing a turnover.

- EFF and OREB (r = 0.717): Efficiency is partly determined by offensive rebounds, so it makes sense that these variables are correlated.

- MIN - REB (r = 0.666), DREB - MIN (r = 0.731), AST - MIN (r = 0.707), MIN - TO (r = 0.663): The more minutes a player spends on the court, the more rebounds, assists, and turnovers they can make.

- 2PT FG - FG (r = 0.780): Usually, players who tend to score more two-point shots will have a higher overall field goal percentage. Notice that FG and 3PT.FG are negatively correlated, since three-point shots are generally more difficult to make and therefore lower the overall field goal percentage.

- DREB and OREB (r = 0.728): If a player is good at grabbing offensive rebounds, they are likely to be good at grabbing defensive rebounds as well.

- DREB - TO (r = 0.666): If a player grabs a defensive rebound, they immediately transition to offense, which increases the chances of losing the ball.

- PTS and TO (r = 0.761), PTS and REB (r = 0.788): The more points a player scores, the more frequently they handle the ball, which increases their probability of losing it. Similarly, the more rebounds a player collects, the higher their chances of scoring.

To summarize the previous ideas, it is clear that a player’s efficiency, the number of points they score, the minutes they spend on the court, and the rebounds they collect are highly correlated. It is also important to note that the MIN variable shows a moderate to strong correlation with almost all numerical variables. This makes complete sense, as the more time a player spends on the court, the higher all their recorded statistics are likely to be. Moreover, since the EFF variable is derived from individual performance indicators, it is expected to be correlated with them. However, because not all statistics contribute equally to a player’s efficiency, the strength of these correlations varies across variables.

```{r}
# Select only the required variables
df2 <- df[, c("Position", "MIN", "FG", "TwoPt.FG", "ThreePt.FG", "FT", "OREB", "DREB", "REB", "AST", "PF", "TO", "STL", "BLK", "EFF", "PTS")]

head(df2)

#Correlation
cor_matrix <- cor(df2[, -1]) 
round(cor_matrix, 2) # correlation matrix

corrplot(cor_matrix, method = "color", type = "upper",
         addCoef.col = "black",     
         tl.col = "black",          
         tl.cex = 0.8,              
         number.cex = 0.7)

plot(df2[, -1])
```

## 2. Hierarchical Clustering

Apply a hierarchical clustering on previously created data frame that includes variables “Position”, “MIN”, “FG”, “2PT.FG”, “3PT.FG”, “FT”, “OREB”, “DREB”, “REB”, “AST”, “PF”, “TO”, “STL”, “BLK”, “EFF” and “PTS”. Scale it if it is required.

```{r}
## Dataframe for clustering
dfclust <- df2

## Scale numerical data
num_vars <- df2[, sapply(dfclust, is.numeric)] # all variables except "Position"

dfscaled <- scale(num_vars)
dfscaled <- as.data.frame(dfscaled) # convert the scaled data to a dataframe
d <- dist(dfscaled, method = "euclidean") # apply distance matrix on the scaled dataframe
```


### a) Decide the best method to draw dendrogram

The hierarchical clustering analysis has been performed on the numerical variables after they have been standardized with the \emph{scale()} function. This ensures that all variables contribute equally to the computation of distances. After the standardization, an Euclidean distance matrix has been obtained using the \emph{dist()} function applied to the scaled dataset.

The clustering structure has been produced with the \emph{hclust()} function, and different linkage criteria have been considered in order to compare their performance. 

The resulting dendrograms allow a first inspection of the potential number of clusters. Using the \emph{rect.hclust()} function, possible partitions into $2$, $3$, $4$, and $5$ clusters are highlighted.

- \textbf{Complete method:} It shows moderately well-defined groups, but some clusters merge at very different heights, producing unbalanced partitions and making the choice of the best k less clear.
- \textbf{Ward method:} It produces the most compact and balanced clusters, with clear separation between them and gradual increases in fusion height, making it the most interpretable and stable method.
- \textbf{Single method:} It suffers from strong chaining, where observations join sequentially at very low heights, resulting in a single elongated cluster and preventing meaningful grouping.
- \textbf{Average method:} It offers a more regular structure than complete or single linkage, but still produces some uneven merges and less compact groups compared with Ward’s method.
- \textbf{Centroid method:} It displays unstable merges and several structural inversions, breaking the expected hierarchy and leading to distorted cluster shapes and reduced interpretability.

```{r}
## Set margins 
par(xpd = TRUE, mar = c(5, 4, 4, 8))

## 1. Complete linkage method
fit1 <- hclust(d, method = "complete")
plot(fit1, main = "Dendrogram of Complete Linkage")

rect.hclust(fit1, k = 2, border = "orange")
rect.hclust(fit1, k = 3, border = "blue")
rect.hclust(fit1, k = 4, border = "red")
rect.hclust(fit1, k = 5, border = "green")

legend("topright", inset = c(-0.25, 0),
       legend = c("2 clusters", "3 clusters", "4 clusters", "5 clusters"),
       col = c("orange", "blue", "red", "green"),
       lwd = 2, cex = 0.8, bg = "white")

## 2. Ward method
fit2 <- hclust(d, method = "ward.D2")
plot(fit2, main = "Dendrogram of Ward Method")

rect.hclust(fit2, k = 2, border = "orange")
rect.hclust(fit2, k = 3, border = "blue")
rect.hclust(fit2, k = 4, border = "red")
rect.hclust(fit2, k = 5, border = "green")

legend("topright", inset = c(-0.25, 0),
       legend = c("2 clusters", "3 clusters", "4 clusters", "5 clusters"),
       col = c("orange", "blue", "red", "green"),
       lwd = 2, cex = 0.8, bg = "white")

## 3. Single linkage method
fit3 <- hclust(d, method = "single")
plot(fit3, main = "Dendrogram of Single Linkage")

rect.hclust(fit3, k = 2, border = "orange")
rect.hclust(fit3, k = 3, border = "blue")
rect.hclust(fit3, k = 4, border = "red")
rect.hclust(fit3, k = 5, border = "green")

legend("topright", inset = c(-0.25, 0),
       legend = c("2 clusters", "3 clusters", "4 clusters", "5 clusters"),
       col = c("orange", "blue", "red", "green"),
       lwd = 2, cex = 0.8, bg = "white")

## 4. Average linkage method
fit4 <- hclust(d, method = "average")
plot(fit4, main = "Dendrogram of Average Linkage")

rect.hclust(fit4, k = 2, border = "orange")
rect.hclust(fit4, k = 3, border = "blue")
rect.hclust(fit4, k = 4, border = "red")
rect.hclust(fit4, k = 5, border = "green")

legend("topright", inset = c(-0.25, 0),
       legend = c("2 clusters", "3 clusters", "4 clusters", "5 clusters"),
       col = c("orange", "blue", "red", "green"),
       lwd = 2, cex = 0.8, bg = "white")

## 5. Centroid linkage method
fit5 <- hclust(d, method = "centroid")
plot(fit5, main = "Dendrogram of Centroid Linkage")

rect.hclust(fit5, k = 2, border = "orange")
rect.hclust(fit5, k = 3, border = "blue")
rect.hclust(fit5, k = 4, border = "red")
rect.hclust(fit5, k = 5, border = "green")

legend("topright", inset = c(-0.25, 0),
       legend = c("2 clusters", "3 clusters", "4 clusters", "5 clusters"),
       col = c("orange", "blue", "red", "green"),
       lwd = 2, cex = 0.8, bg = "white")

par(xpd = FALSE, mar = c(5, 4, 4, 2))
```


### b) How many clusters do you think exist in the data considering the dendrogram?

Based on the Ward dendrogram, a solution with \textbf{$4$ clusters} appears to be the most appropriate. The dendrogram is cut at a level where four groups remain clearly separated and internally coherent, while avoiding the merging of clusters that are substantially different. A cut at $3$ clusters would still merge branches that show noticeable internal variability, while increasing the number of clusters to $5$ would lead to an unnecessary fragmentation of already compact groups. 
For this reason, the dendrogram suggests that the underlying structure of the data is best described by \textbf{$4$ clusters}.

## 3. K means clustering

### a) Find the optimum number of clusters

A non-hierarchical clustering analysis has been performed using several criteria to determine the optimum number of clusters. Three different methods are considered: the \textbf{TWSS Elbow Graph}, the \textbf{Pseudo F Index}, and the \textbf{Silhouette Index}.

#### I) TWSS Elbow Graph

The Total Within-Cluster Sum of Squares (TWSS) has been computed for different values of k using the \textbf{k-means algorithm}, in order to identify an appropriate number of clusters through the \textbf{Elbow method}.

The TWSS curve shows a clear elbow at $k=4$, where the reduction in within-cluster variability becomes noticeably less steep.
Moreover, the ratio between the between-cluster sum of squares and the total sum of squares increases from $0.437$ for $k=3$ to $0.544$ for $k=4$, indicating a  better separation between groups. Therefore, based on the Elbow method and the explained separation, $k=4$ is selected as the optimal number of clusters.

```{r}
## Applying k-means algorithm for different number of clusters

aux <- c()
for (i in 2:6){
  k <- kmeans(dfscaled, centers = i, nstart = 25)
  aux[i-1] <- k$tot.withinss 
}

plot(2:6, aux,
     xlab = "Number of Clusters",
     ylab = "TWSS",
     type = "l",
     main = "TWSS vs. number of clusters")

points(4, aux[4 - 1], pch = 19, cex = 1.5, col = "red") # Optimum point: k=4

# k=3
k3 <- kmeans(dfscaled, centers = 3, nstart = 25) # 3 number of clusters

## Sum of Squares
k3$withinss
k3$totss
k3$tot.withinss # total deviation
k3$betweenss + k3$tot.withinss # total variation of the dataset
k3$betweenss/k3$totss # 0.437 separation between clusters

# k=4
k4 <- kmeans(dfscaled, centers = 4, nstart = 25) # 4 number of clusters 

## Sum of Squares 
k4$withinss 
k4$totss 
k4$tot.withinss # total deviation
k4$betweenss + k4$tot.withinss # total variation of the dataset
k4$betweenss/k4$totss # 0.544 greater separation between clusters
```

#### II) Pseudo F Index

The \textbf{Pseudo-F Index} has been computed for different values of k in order to evaluate the separation between clusters obtained with k-means algorithm. As shown in the figure, the index reaches its maximum value at $k=4$ and then decreases steadily as k increases. 

```{r}
# Pseudo F index = BSS*(n-k)/WSS*(k-1)
aux <- c()
for (i in 2:10){
  k <- kmeans(dfscaled, centers=i, nstart=25)
  aux[i-1] <- ((k$betweenss)*(nrow(dfscaled)-i))/((k$tot.withinss)*(i-1))
}
plot(2:10, aux, xlab="Number of Clusters", ylab="Pseudo-F", type="l", main="Pseudo F Index")
points(4, aux[4 - 1], pch = 19, cex = 1.5, col = "red")
# max Pseudo F: 4 clusters
```

#### III) Silhoutte Index

Finally, the \textbf{Silhouette Index} has been computed using both the \emph{silhouette()} function from the \emph{cluster} package and the \emph{fastkmed()} function from the \emph{kmed} package.

Using the \emph{silhouette()} function, the average silhouette widths obtained for $k=3$ and $k=4$ are $0.22$ and $0.21$, respectively. Although $k=3$ achieves a slightly higher value, the difference is marginal and both solutions exhibit comparable cohesion. 
In contrast, the silhouette analysis based on \emph{fastkmed()} indicates better performance for $k=4$, with higher and more stable silhouette values across clusters.

Overall, the Silhouette Index does not provide a strong preference for either solution, but it does not contradict the four-cluster partition. Given the clearer separation indicated by the TWSS Elbow method and the Pseudo-F Index, $k=4$ is the most meaningful partition. The four-cluster solution provides a better separation between groups ($BSS/TSS=0.544$), a clearer structural interpretation, and it is fully consistent with the hierarchical clustering results. For these reasons, $k=4$ has been selected as the optimal number of clusters.

```{r}
## A) Silhouette function from cluster library
# k=3
si <- silhouette(k3$cluster, d)
plot(si) # higher average width

# k=4
si <- silhouette(k4$cluster, d)
plot(si)


## B) Silhouette function from kmed library
# k=3
res <- fastkmed(d, 3) 
silhouette <- sil(d, res$medoid, res$cluster)
silhouette$result
silhouette$plot

# k=4
res <- fastkmed(d, 4)
silhouette <- sil(d, res$medoid, res$cluster)
silhouette$result
silhouette$plot
# k=4 higher silhouette value
```

### b) Apply k-means clustering using the number of clusters that you decided. Interpret each cluster by making a descriptive statistics table

Based on the results obtained in the previous sections, $k=4$ has been selected as the optimal number of clusters, and the k-means algorithm has been applied to the scaled dataset. The resulting clusters have been interpreted using descriptive statistics as follows:


\textbf{- Cluster 1: Low-Impact Bench Players}

This cluster is composed of $22.4\%$ of the players. It represents the lowest-impact players, characterized by extremely limited playing time (with only $\approx 9.65$ minutes) and minimal statistical production across all categories. The cluster is dominated by centers ($36.4\%$) and forwards ($45.5\%$), with only 2 guards ($18.2\%$), reflecting an interior-oriented composition. While they show relatively high two-point shooting ($\approx 79.9\%$), this reflects shot selection rather than offensive skill, as evidenced by their very poor three-point shooting and free throw shooting ($\approx 11.4\% \text{ and } \approx 1.49\%$, respectively). Their contributions to rebounds, assists, steals, and blocks are all minimal, resulting in the lowest efficiency rating and scoring output. These are, indeed, deep bench reserves, who see action only in limited situations to provide size or rest for starters.

\textbf{- Cluster 2: Perimeter Role Players}

This cluster is composed of $36.7\%$ of the players. It captures perimeter-oriented players with moderate playing time ($\approx 14.8$ minutes) who specialize in outside shooting. The cluster is heavily dominated by guards ($50.0\%$) and forwards ($38.9\%$), reflecting a clear perimeter orientation. Their profile shows below-average overall field goal percentage and poor two-point shooting, but respectable three-point shooting ($\approx 34.8\%$), indicating they operate primarily away from the basket. Their free throw shooting is moderate, and they contribute modestly to assists and steals, but provide minimal interior presence with very low blocks and rebounds. With $4.99$ points and $5.69$ efficiency, they represent bench-level perimeter specialists. In other words, primarily guards who enter games to provide outside shooting but lack the overall impact or minutes of rotation starters.

\textbf{- Cluster 3: Balanced starters with strong contribution}

This cluster is composed of $34.7\%$ of the players. It represents high-quality rotation players who receive substantial playing time ($\approx 24.2$ minutes) and contribute across multiple dimensions. The cluster shows a pretty balanced positional composition with guards ($52.9\%$), forwards ($41.2\%$) and centers ($5.9\%$), suggesting these are versatile players regardless of nominal position. They show balanced shooting profiles with moderate overall field goal percentage, acceptable two-point shooting, and strong three-point shooting ($\approx 43.2\%$), indicating versatility in shot selection. Their excellent free throw shooting ($\approx 79.2\%$) reflects technical skill, and they contribute significantly to assists, rebounding, and defensive activity. With $11.3$ points and $13.3$ efficiency, these are the main rotation players: reliable, versatile contributors who can impact the game in multiple ways and handle significant minutes, though they are not the primary offensive options.

\textbf{- Cluster 4: Elite Performers}

This cluster is composed of $6.2\%$ of the players. It contains the highest-impact players, characterized by heavy playing time ($\approx 29.7$ minutes) and dominance across virtually all statistical categories. Interestingly, this elite cluster contains only forwards, with no guards or centers. They excel in scoring, efficiency ($\approx 28.1$), and rebounding ($\approx 9.60\%$ total rebounds, with exceptional offensive rebounding at $\approx 3.20\%$). They also contributing significantly to playmaking ($4.30$ assists), steals, and blocks. Their shooting profile shows strong two-point percentage and good free throw shooting, though their three-point percentage is lower, likely because they operate in diverse scoring areas rather than specializing from distance. The elevated turnover rate ($\approx 3.07$) reflects their high usage and ball-handling responsibilities rather than poor decision-making. These are elite players who carry their teams' offensive and defensive responsibilities, dominating statistical production across the board.

```{r}
# Apply k-means with the selected number of clusters
k4 <- kmeans(dfscaled, centers = 4, nstart = 25)

# Add cluster label to original unscaled data
dfclust$Cluster <- as.factor(k4$cluster)

# Descriptive statistics per cluster
cluster_summary <- dfclust[, -1] %>%
  group_by(Cluster) %>%
  summarise(across(everything(), mean, na.rm = TRUE))
cluster_summary

# Clusters based on factor Position
table(dfclust$Cluster, dfclust$Position)
```

## 4. Hierarchical Clustering based on PCA and MCA

In order to proceed with the hierarchical clustering based on PCA and MCA, the PCA and MCA results from Homework 1 have been computed using the \emph{PCA()} and \emph{MCA()} functions from the library \emph{FactoMineR}. Note that the PCA has been computed taking the Position as a supplementary qualitative variable and the EFF as supplementary quantitative variable. On the other hand, the MCA has been applied to the \emph{df_binary} that categorized the numeric variables.

```{r}
dfhclust <- df2

df_binary <- dfhclust

binary <- function(x){
  ifelse(x > mean(x), "over_average", "below_average")
}

for(col in names(dfhclust[,-1])) {
  df_binary[[col]] <- as.factor(binary(dfhclust[[col]]))
}


eff_idx <- which(colnames(dfhclust) == "EFF")
pos_idx <- which(colnames(dfhclust) == "Position")

res.pca <- PCA(dfhclust,
           scale.unit = TRUE,
           quali.sup  = pos_idx,
           quanti.sup = eff_idx,
           graph = FALSE)

res.mca <- MCA(df_binary,
               graph = FALSE)
```


### a) Use HCPC() function and apply a PCA based clustering on the pca output that you had from HW1. Decide on number of clusters

The \emph{HCPC()} function has been applied to the PCA results. This approach allows to identify groups of players with similar performance profiles based on the principal components rather than the original correlated variables, ensuring that each dimension contributes equally to the cluster formation.

The function allows direct identification of the optimal number of clusters by analyzing the inertia gain at each hierarchical partition. However, the dendrogram should be examined to ensure the selected solution produces interpretable and meaningful clusters with balanced sizes. In this case, $k=3$, as recommended by the HCPC algorithm, represents the optimal number of clusters, maximizing between-cluster separation while maintaining interpretable player profiles. Note that fewer clusters would merge the two big groups together and lose important patterns, while adding clusters only partitions groups that could be considered a single cohesive segment.

The only potential issue is the presence of one unbalanced cluster, corresponding to the first split in the dendrogram. However, this smaller cluster is meaningful, as it contains the exceptional players identified as outstanding performers in the PCA analysis. These elite players score extremely high on Dimension 1 (overall performance), clearly distinguishing them from the rest. Thus, although numerically smaller, the cluster represents a statistically significant and basketball-relevant group of star players.

```{r}
res.hcpc.pca <- HCPC(res.pca, nb.clust = -1, consol = TRUE, graph = FALSE)

# Visualize the dendrogram
fviz_dend(res.hcpc.pca, rect = TRUE, rect_fill = TRUE, main = "Dendrogram - PCA-based Hierarchical Clustering")

# Plot the inertia gain to visualize cluster selection
plot(res.hcpc.pca, choice = "bar", main = "Inertia gain on PCA")

# Visualize clusters in the PCA factor map
fviz_cluster(res.hcpc.pca, repel = TRUE, show.clust.cent = TRUE,main = "Cluster Visualization on PCA Factor Map (Dim 1 vs Dim 2)")

# 3D visualization
plot(res.hcpc.pca, choice = "3D.map", main = "3D Cluster Visualization")
```

### b) Interpret each cluster by using profiling

The PCA-based hierarchical clustering procedure identifies three distinct groups of players with clearly differentiated performance profiles. On the one hand, a chi-square test revealed a significant association between cluster membership and playing position, indicating that the null hypothesis of independence between cluster assignment and position is rejected. Moreover, the ANOVA results support highly significant differences across all major performance variables. Specifically, the null hypothesis of equal means across clusters is rejected for all numerical variables except for the overall field goal percentage. Variables showing the strongest discriminatory power include efficiency, total rebounds, points, and minutes played.

\textbf{- Cluster 1: Bench Players and Low-Impact Centers}
This cluster comprises $13$ players ($26.5\%$ of the sample).

Centers are significantly over-represented in this cluster, comprising $38.5\%$ of the group compared to only $14.3\%$ in the overall sample. Moreover, $71.4\%$ of all Centers in the sample belong to this cluster.

Players in this cluster are characterized by significantly below-average mean values across the vast majority of performance metrics. Most notably, they play dramatically fewer minutes than the overall sample, which directly impacts their accumulated statistics. Their scoring output is minimal, as is their overall efficiency rating.

The shooting profile of these players reveals a pronounced limitation in perimeter skills, with extremely low three-point field goal percentages and poor free throw shooting. However, they demonstrate relatively strong two-point shooting percentages, suggesting their offensive contributions are limited to close-range attempts near the basket. These is consistent with the fact that centers are usually the players that take two-point shoots.

Therefore, this cluster represents the \textit{deep bench players}. In other words, backup players, mostly centers, who play only a few minutes per game. These players enter the court briefly to give the starting players rest or to address specific game situations.

\textbf{- Cluster 2: Perimeter Shooting Specialists}

This is the largest cluster, containing $33$ players ($67.3\%$ of the sample). 

Guards are significantly over-represented in this cluster, comprising $51.5\%$ of the group compared to $40.8\%$ in the overall sample. Remarkably, $85\%$ of all Guards belong to this cluster, indicating a strong association between the guard position and this performance profile. Centers, conversely, are notably under-represented.

Players in this cluster display a clear perimeter-oriented profile. Their most defining characteristic is strong three-point shooting, indicating specialization in outside shooting. This is complemented by above-average free throw shooting and number of assistances, which typically correlates with perimeter shooting ability. Conversely, these players show below-average two-point field goal percentages and overall field goal percentages, since three-point shootings are most commonly failed. Notice that the players of these cluster also play more minutes, so it can be seen as the standard rotation players. 

Therefore, this cluster represents primarily guards who contribute through perimeter shooting and ball movement rather than all-around production. They represent the middle class of professional basketball.

\textbf{- Cluster 3: Elite Performers}

This cluster contains only $3$ players ($6.1\%$ of the sample), representing an extremely selective group of exceptional performers who stand apart from the rest of the tournament participants.

Players in this cluster exhibit elite-level performance across almost every statistical category, demonstrating all-around excellence rather than narrow specialization. NOtice that this cluster is composed of the players who stood out in the PCA factor map, those who excelled in overall performance and workload and playing style.

Interestingly, no single playing position is significantly over-represented in this cluster. This suggests that these players are exceptional regardless of whether they are classified as guards, forwards, or centers.

Hence, this cluster is represented by the \textbf{elite} players.

It should be noted that the forward position is distributed relatively evenly across all three clusters, showing no significant association with any particular performance profile. This indicates that the forward position encompasses a wide range of player types, from deep bench reserves to elite players.

In conclusion, as observed on the Factor Map, the first cluster corresponds to players with low overall performance and productivity, located on the left side of the first dimension axis. The second cluster consists of players with average overall performance and productivity, positioned near the center of the graph. Finally, the third cluster includes the outstanding players, who exhibit the highest overall performance and productivity.

```{r}
table(res.hcpc.pca$data.clust$clust)
prop.table(table(res.hcpc.pca$data.clust$clust))

# Cluster description: categorical variables
res.hcpc.pca$desc.var$test.chi2
res.hcpc.pca$desc.var$category

# Cluster description: numerical variables
res.hcpc.pca$desc.var$quanti.var
res.hcpc.pca$desc.var$quanti
```

### c) Use HCPC() function and apply a MCA based clustering on the multiple correspondence analysis output that you had from HW1. Decide on number of clusters

The \emph{HCPC()} function has been applied to the MCA results. This approach allows to identify groups of players with similar categorical performance patterns based on the MCA dimensions.

The algorithm again selects $k=3$ as the optimal number of clusters. This choice is fully consistent with the inertia-gain bar plot, which displays a clear elbow at $k=3$. Inspection of the dendrogram further supports this result: using one fewer cluster would merge the two largest groups into an overly heterogeneous cluster combining players with distinct categorical profiles, whereas increasing to $k=4$ would split the green cluster, producing an excessively small and unbalanced group.

```{r}
res.hcpc.mca <- HCPC(res.mca, nb.clust = -1, consol = TRUE, graph = FALSE)

# Visualize the dendrogram
fviz_dend(res.hcpc.mca, rect = TRUE, rect_fill = TRUE, main = "Dendrogram - MCA-based Hierarchical Clustering")

# Plot the inertia gain to visualize cluster selection
plot(res.hcpc.mca, choice = "bar", main = "Inertia gain on MCA")

# Visualize clusters in the PCA factor map
fviz_cluster(res.hcpc.mca, repel = TRUE, show.clust.cent = TRUE,main = "Cluster Visualization on MCA Factor Map (Dim 1 vs Dim 2)")

# 3D visualization
plot(res.hcpc.mca, choice = "3D.map", main = "3D Cluster Visualization")
```

### d) Interpret each cluster by using profiling

The MCA-based hierarchical clustering procedure identifies three distinct groups of players based on their categorical performance patterns. The validity of this partition is supported by highly significant chi-square tests of independence for all performance categories, indicating that the null hypothesis of equal distribution across clusters is rejected for all categorized performance variables. In addition, playing position is significantly associated with cluster assignment, further supporting the meaningfulness of the identified groups.

\textbf{- Cluster 1: Bench Players and Low-Impact Centers}

This cluster comprises $18$ players ($36.7\%$ of the sample).

Centers are significantly over-represented in this cluster, comprising $33.33\%$ of the group compared to only $14.3\%$ in the overall sample. This indicates that $85.5\%$ of all centers in the tournament belong to this cluster.

Players in this cluster consistently fall below the sample average across most performance metrics. For instance, $94.44\%$ of players in the cluster have below-average minutes played. As expected, this limited playing time extends to other performance dimensions: below-average categories for PTS, TO, EFF, AST, and STL are all strongly over-represented in this cluster, indicating that these centers generally perform below the sample average in most metrics. Consequently, it is logical that this cluster is under-represented in the above-average categories for the same performance variables. Interestingly, the only performance dimension in which they are over-represented is two-point shooting, which aligns with the typical role of centers, who often focus on scoring near the basket rather than other aspects of the game.

Hence, this cluster represents \textit{bench players with minimal court impact}, predominantly backup centers who receive limited playing time and contribute minimally to team statistics, primarily through two-point scoring near the basket.

\textbf{- Cluster 2: Perimeter Shooting Specialists}

This cluster contains $19$ players ($38.8\%$ of the sample).

Guards dominate this cluster, comprising $73.68\%$ of the group compared to $40.8\%$ in the overall sample. Remarkably, $70\%$ of all guards in the tournament belong to this cluster. Conversely, centers are completely absent from this cluster, and forwards are under-represented.

The players in this cluster are clearly over-represented by above average perimeter skills and below average interior contributions. Players in this cluster exhibit exceptional perimeter specialization. Remarkably, $100\%$ of cluster members shoot above average from three-point range. This three-point excellence extends to free throw shooting, where $84.2\%$ shoot above average, reflecting consistent shooting mechanics. However, this perimeter orientation creates systematic deficiencies in interior play. Nearly all players shoot below average on two-point attempts. Similarly, the vast majority fall below average in rebounds (total, offensive, and defensive), blocks, and overall field goal percentage, indicating minimal contribution in areas requiring interior presence or physical play near the basket. Consequently, this cluster shows corresponding under-representation in above-average interior categories.

Hence, this cluster represents \textbf{perimeter specialists}, whose value comes almost entirely from outside shooting. These players space the floor, shoot open three-pointers efficiently, and contribute primarily through perimeter skills. However, they provide minimal impact in the interior game. This pattern aligns perfectly with the typical role of guards, who focus on outside scoring, playmaking, and spacing rather than interior presence.

\textbf{- Cluster 3: Elite Performers}

This cluster contains $12$ players ($24.5\%$ of the sample).

Forwards are significantly over-represented in this cluster, comprising $75\%$ of the group. In fact, $41\%$ of the forwards in the sample are represented in this group.

Players in this cluster exhibit systematic above-average performance across the vast majority of statistical categories, demonstrating all-around excellence. For example, all players with above-average REB, DREB, EFF and MIN are represented in this cluster, indicating a strong association between high performance in these metrics and this cluster profile. Every player in this cluster exceeds the sample mean on these fundamental measures of impact and usage. As expected, they are under-represented on the below-average category for these variables. The only exceptions are three-point shooting and overall field goal percentage, where the cluster does not show particularly high representation. This indicates that these elite players achieve their dominance through overall contribution and effectiveness across multiple aspects of the game rather than through exceptional shooting efficiency alone.

This aligns with basketball logic. Forwards often record above-average statistics across multiple categories because they operate in different areas of the court: they can rebound like centers, assist like guards, and score from various distances. Guards tend to specialize in assists and perimeter shooting but contribute less to rebounding and interior defense, while centers dominate rebounding and blocks but have less impact on assists and outside scoring. Positioned between these extremes, forwards are able to perform above average across a wide range of statistical categories. Therefore, this cluster comprises \textit{elite all-around performers}, who contribute across multiple dimensions and carry significant offensive and defensive responsibilities.


```{r}
table(res.hcpc.mca$data.clust$clust)
prop.table(table(res.hcpc.mca$data.clust$clust))

# Cluster description: categorical variables
res.hcpc.mca$desc.var$test.chi2
res.hcpc.mca$desc.var$category
```

## 5. Conclusion
### Compare the results of clustering methods that you applied and make a final comment on which clustering method would you prefer and why

Several clustering methods have been applied to the player performance data. While each method provides valuable insights, a comparison reveals important differences.

K-means clustering identifies four distinct groups, separating low-impact bench players, perimeter role players, balanced starters, and elite performers. This method captures numeric differences in performance metrics well, but produces clusters of highly unequal size and is sensitive to extreme outliers, such as exceptional players with unusually high minutes or efficiency. In addition, some groups, like the balanced starters, could be redistributed across other clusters, reducing interpretability.

PCA-based hierarchical clustering reduces dimensionality and highlights numeric patterns, resulting in three clusters. It successfully separates elite players, shooters, and low-profile players, but it is less able to separate players according to their positions, as forwards are spread across multiple clusters. Moreover, cluster sizes are uneven and highly sensitive to extreme performers.

MCA-based hierarchical clustering also identifies three clusters, but has several advantages over the other approaches. The clusters are more balanced in size, naturally separate players according to their positions, and clearly show how performance metrics relate to position-specific roles. Additionally, MCA is less influenced by extreme outliers, focusing on over/below average patterns absolute than raw numerical values. 

Considering these factors, MCA-based clustering is preferred for this dataset because it produces balanced, robust, and interpretable clusters. It effectively identifies bench players, perimeter specialists, and elite all-around performers, with descriptions that align closely with basketball roles.

## 6. Discriminant Analysis

Our objective is to fit a prediction model to classify players according to their positions.

### a) Consider only the variables that have approximately normal distribution and test the assumption of homogenity of variance

After examining the histograms, QQ plots, and assessing normality using the Shapiro–Wilk and Kolmogorov tests, the variables that exhibit an approximately normal distribution are MIN, FG, TwoPt.FG, and PF. This conclusion is based on their bell-shaped histograms and QQ plots that approximately follow the diagonal line, in contrast to the remaining variables, which clearly deviate from normality.

This is also supported by the Shapiro–Wilk normality test, which rejects the null hypothesis for all variables except:
- MIN, with a p-value of 0.1948
- TwoPt.FG, with a p-value of 0.1178
- PF, with a p-value of 0.3919

FG does not pass the Shapiro–Wilk normality test, as its p-value is 0.01795. However, according to the Kolmogorov test, normality is not rejected. Combined with the fact that its histogram is approximately bell-shaped, FG is therefore considered approximately normal for the purposes of this analysis.

```{r}
# Assumption 1: Normality

## Checking histogram, qqplots and shapiro Willk's test results
for (i in 2:16){
  print(colnames(df2)[i])
  print(shapiro.test(df2[,i]))
  print(ks.test(df2[,i], "pnorm", mean=mean(df2[,i]), sd=sd(df2[,i])))
qqnorm(df2[,i],main=colnames(df2)[i])
hist(df2[,i], main=colnames(df2)[i])
}
```

Since we will work only with the approximately normal variables, the variables we will use for this analysis are: MIN, FG, TwoPt.FG and PF.  
```{r}
## Dataframe for DA
dfdisc <- df2[,c(1:4,11)]
```

Then, to assess the homogeneity of variances across the different positions for the normally distributed variables, Box’s M test is applied to evaluate the equality of variance-covariance matrices using the \emph{boxM()} function.

As the resulting p-value is significant (0.00225), the null hypothesis of homogeneity of variances is rejected. Furthermore, inspection of the boxplots reveals clear differences in variance homogeneity across all variables.

```{r}
# Assumption 2: Homogeneity of Variance

## Box's M test for testing variance covariance matrices
# install.packages("biotools")
library(biotools)
?boxM

boxM(dfdisc[,2:5],dfdisc$Position)
boxplot(dfdisc[,2]~dfdisc$Position, main="MIN") # no variance homogeneity in MIN
boxplot(dfdisc[,3]~dfdisc$Position, main="FG") # no variance homogeneity in FG
boxplot(dfdisc[,4]~dfdisc$Position, main="TwoPt.FG") # no variance homogeneity in TwoPt.FG
boxplot(dfdisc[,5]~dfdisc$Position, main="PF") # no variance homogeneity in PF
```

```{r}
# Correlation
cor(dfdisc[,2:5])
plot(dfdisc[,2:5])
```

###  b) Fit the proper discriminant model on the data set and explain why you use it
There are three main types of discriminant models:

- Linear Discriminant Analysis (LDA), which assumes that the predictors follow a normal distribution and that all classes share the same covariance matrix (homogeneity of variances).
- Quadratic Discriminant Analysis (QDA), which assumes normality of the predictors but allows each class to have its own covariance matrix.
- Naive Bayes, which assumes conditional independence between predictors.

In this data set, the variables can reasonably be approximated by normal distributions. However, the assumption of homogeneity of variances across classes is not satisfied, which violates a key requirement of LDA. In addition, the predictors cannot be considered independent, as there are strong correlations between some variables (for example, FG and TwoPt.FG show a correlation of 0.78), making Naive Bayes inappropriate.

Therefore, QDA is the most suitable discriminant model, as it relaxes the equal covariance assumption while still accommodating normally distributed variables. We fit the model using all the variables that were identified as approximately normal, applying the \emph{qda()} function.

```{r}
qda.fit<-qda (Position ~ MIN + TwoPt.FG + PF + FG , data = dfdisc)
qda.fit$prior
```


### c) Make a table cross classifying real and predicted values and interpret correct classification rates

The confusion matrix is shown where rows correspond to the true classes and columns to the predicted classes.

The confusion matrix reveals a clear performance disparity across positions, resulting in an overall Correct Classification Rate (CCR) of 75.5%. The model performs best when classifying Guards, correctly identifying 18 out of 20 observations, resulting in a class-specific CCR of 90%. In contrast, performance drops significantly for the frontcourt positions: Forwards were classified with 68.18% accuracy (15/22), while Centers proved the most difficult, with only 4 out of 7 correctly identified (57.14%), being misclassified as Forwards.

These observed misclassifications are primarily driven by the variable selection, which restricted the model to MIN, FG, 2PT FG, and PF. By excluding critical discriminators like Rebounds (REB) and Assists (AST), the model lacks the necessary metrics to distinguish between roles. The following conclusions are extracted from the analysis of the misclassified players: 

  - Center vs. Forward Confusion: This confusion stems from the lack of statistical separation between the roles in the selected variables. As shown in the summary table, Centers and Forwards share very similar Field Goal values, making them indistinguishable by efficiency.

  - Forward vs. Guard Confusion: The 6 misclassified Forwards are perimeter-oriented players whose statistics mimic those of a Guard. Comparing their values against the boxplots shows unusually low Personal Fouls and lower Field Goal values due to outside shooting. Without the Rebounds variable, the model interprets these stats (lacking inside dominance) as characteristic of Guards.

Nevertheless, despite these variable limitations, the model represents a substantial improvement over random chance. The calculated baseline accuracy, based on class proportions, is only 38.86%. Therefore, the model's CCR of 75.5% confirms that the discriminant functions successfully capture meaningful patterns in the data, even without the full statistical profile of the players.


```{r}
pred <- predict(qda.fit, dfdisc)$class

error_analysis <- data.frame(
  Player = rownames(dfdisc),
  Actual = dfdisc$Position,
  Predicted = pred,
  PF = dfdisc$PF,
  MIN = dfdisc$MIN,
  FG = dfdisc$FG,
  TwoPt = dfdisc$TwoPt.FG
)

cat("\n--- FORWARDS MISCLASSIFIED AS GUARDS ---\n")
F_to_G <- subset(error_analysis, Actual == "Forward" & Predicted == "Guard")
print(F_to_G)

mean_PF_Guard <- mean(dfdisc$PF[dfdisc$Position == "Guard"])
cat("\nMean PF for true Guards:", round(mean_PF_Guard, 2), "\n")

cat("\n--- CENTERS MISCLASSIFIED AS FORWARDS ---\n")
C_to_F <- subset(error_analysis, Actual == "Center" & Predicted == "Forward")
print(C_to_F)

boxplot(MIN ~ Position, data = dfdisc, main = "MIN", xlab = "Position", ylab = "MIN")
boxplot(FG ~ Position, data = dfdisc, main = "FG", xlab = "Position", ylab = "FG")
boxplot(TwoPt.FG ~ Position, data = dfdisc, main = "TwoPt.FG", xlab = "Position", ylab = "TwoPt.FG")
boxplot(PF ~ Position, data = dfdisc, main = "PF", xlab = "Position", ylab = "PF")

```


```{r}
# Group predictions
pospred <- predict(qda.fit)

# Class and posterior probability predictions 
pospred$class
pospred$posterior

# Contingency Table of Observed and Predicted Values
tab2<-table(dfdisc$Position,pospred$class) #columns are predictions
tab2

# CCR (Correct Classification Rate)
classrate<-sum(diag(tab2))/sum(tab2)
classrate

# CCR among groups
diag(prop.table(tab2, 1))

# Prediction accuracy p1^2+p2^2+p3^2
qda.fit$prior[1]^2 + qda.fit$prior[2]^2+qda.fit$prior[3]^2
```
MIN, FG, TwoPt.FG, and PF

els forwards com poden defensar a tothom son els que fan mes faltes (poden defensar als centers q com son mes gran faran faltes per pararlos, i els altres que com els forwards son mes rapids q ells)

fan moltes faltes per defensar als guards de laltre equip i per parar als centers de lequip contrari





### d) Test whether the classification is done by chance using Q statistic

To assess whether the classification accuracy of the discriminant function is meaningfully better than random classification, we use the Q statistic. This statistic evaluates whether the number of correctly classified observations exceeds what would be expected by chance. Under the null hypothesis, the discriminant function has no real discriminatory power and its classification performance is equivalent to random guessing.

The Q statistic follows a chi-squared distribution with $k - 1$ degrees of freedom, where $k$ is the number of classes, and the test is conducted at a chosen significance level $\alpha$. A large value of Q provides evidence against the null hypothesis, indicating that the classifier performs significantly better than chance.

In our case, the test shows a p-value of $3.04e-09$, therefore we reject the null hypothesis. This result indicates that the observed improvement in prediction is statistically significant when compared to random classification under equal class priors. 

We also compute the critical value of the chi-squared distribution at different significance levels with $k - 1$ degrees of freedom. since the observed Q statistic is greater than the critical value  at different significance levels, the test statistic falls within the rejection region, further confirming that the null hypothesis is rejected.

```{r}
n <- sum(tab2) # Total number of observations
k <- nrow(tab2) # Number of classes (number of rows in the confusion matrix)
ntilde <- sum(diag(tab2)) # Number of correctly classified observations (sum of the diagonal)

# Q statistic as defined in the lecture notes:
Q <- (n - ntilde*k)^2 / (n * (k - 1)); Q

# p-value using chi-squared distribution with (k - 1) degrees of freedom
pvalue <- 1 - pchisq(Q, df = k - 1); pvalue

crit <- qchisq(0.9, df = k - 1); 
Q > crit # TRUE

crit <- qchisq(0.95, df = k - 1); 
Q > crit # TRUE

crit <- qchisq(0.99, df = k - 1); 
Q > crit # TRUE
```
